{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb4916c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "from scipy.io import loadmat, savemat\n",
    "from PIL import Image\n",
    "from tqdm import tqdm    # progress bar\n",
    "\n",
    "MAX_IMAGES = 20000 # set MAX_IMAGES to -1 for unlimited.\n",
    "\n",
    "SYNTH_ROOT   = \"datasets/SynthText\"\n",
    "ORIG_GT      = os.path.join(SYNTH_ROOT, \"gt.mat\")\n",
    "RESIZED_ROOT = \"datasets/SynthText_sub_resized_4yolo\" #currently named as it should\n",
    "IMG_DIR      = os.path.join(RESIZED_ROOT, \"images\")\n",
    "RESIZED_GT   = os.path.join(RESIZED_ROOT, \"gt.mat\")\n",
    "\n",
    "# only digits + ASCII letters\n",
    "ALLOWED = set(string.digits + string.ascii_letters)\n",
    "\n",
    "# target size\n",
    "TARGET_SIZE = (448, 448)\n",
    "\n",
    "os.makedirs(IMG_DIR, exist_ok=True) #makes the directory if it doesnt exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1afeb0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chars(txt_list):\n",
    "    \"\"\"\n",
    "    Function name: extract_chars\n",
    "    Description: Given a list of raw substrings, return all non-whitespace characters in order.\n",
    "    Parameters:\n",
    "        txt_list (list[str]): List of substrings or single string segments from gt['txt'].\n",
    "    Return Value:\n",
    "        list[str]: List of characters from all segments, excluding spaces or line breaks.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for seg in txt_list:\n",
    "        for c in str(seg):\n",
    "            if not c.isspace():\n",
    "                out.append(c)\n",
    "    return out\n",
    "\n",
    "def is_allowed(chars, allowed_set=ALLOWED):\n",
    "    \"\"\"\n",
    "    Function name: is_allowed\n",
    "    Description: Check whether every character in the list is in the allowed set.\n",
    "    Parameters:\n",
    "        chars (list[str]): List of characters to validate.\n",
    "        allowed_set (set[str]): Set of permitted characters (digits + ASCII letters).\n",
    "    Return Value:\n",
    "        bool: True if all characters are in allowed_set, False otherwise.\n",
    "    \"\"\"\n",
    "    return all(c in allowed_set for c in chars)\n",
    "\n",
    "def split_and_clean(substrings):\n",
    "    \"\"\"\n",
    "    Function name: split_and_clean\n",
    "    Description: Split raw substrings on whitespace and strip each piece to form clean words.\n",
    "    Parameters:\n",
    "        substrings (list[str]): Raw segments from gt['txt'], possibly containing spaces or newlines.\n",
    "    Return Value:\n",
    "        list[str]: List of cleaned words with no leading/trailing whitespace.\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    for seg in substrings:\n",
    "        for w in str(seg).split():\n",
    "            cw = w.strip()\n",
    "            if cw:\n",
    "                words.append(cw)\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13ac8878",
   "metadata": {},
   "outputs": [],
   "source": [
    "data     = loadmat(ORIG_GT, squeeze_me=True, struct_as_record=False)\n",
    "imnames  = list(data['imnames'].flatten())   # e.g. \"8/ballet_106_38.jpg\"\n",
    "charBBs  = list(data['charBB'].flatten())     # each shape (2,4,n_chars)\n",
    "wordBBs  = list(data['wordBB'].flatten())     # each shape (2,4,n_words)\n",
    "raw_txts = list(data['txt'].flatten())        # each is np.ndarray or str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7adaf0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resizing...:  10%|█████▉                                                       | 82834/858750 [03:23<31:47, 406.80it/s]\n"
     ]
    }
   ],
   "source": [
    "new_imnames, new_charBB, new_wordBB, new_txt = [], [], [], []\n",
    "\n",
    "for idx in tqdm(range(len(imnames)), desc=\"Resizing...\"):\n",
    "    name    = imnames[idx]\n",
    "    raw_txt = raw_txts[idx]\n",
    "    # 1. turn raw_txt into a Python list of substrings\n",
    "    if isinstance(raw_txt, np.ndarray):\n",
    "        substrs = [str(x) for x in raw_txt.flatten()]\n",
    "    else:\n",
    "        substrs = [str(raw_txt)]\n",
    "\n",
    "    # 2. flatten all chars & filter\n",
    "    chars = extract_chars(substrs)\n",
    "    if not is_allowed(chars):\n",
    "        continue\n",
    "\n",
    "    bb  = charBBs[idx]\n",
    "    if bb.shape[2] != len(chars):\n",
    "        continue\n",
    "\n",
    "    # 3. open + resize image\n",
    "    src_path = os.path.join(SYNTH_ROOT, name)\n",
    "    if not os.path.isfile(src_path):\n",
    "        continue\n",
    "    img       = Image.open(src_path).convert(\"RGB\")\n",
    "    w0, h0    = img.size\n",
    "    scale_x   = TARGET_SIZE[0] / w0\n",
    "    scale_y   = TARGET_SIZE[1] / h0\n",
    "    img_res   = img.resize(TARGET_SIZE, Image.BILINEAR)\n",
    "\n",
    "    # 4. scale charBB\n",
    "    bb2        = bb.astype(float)\n",
    "    bb2[0, :, :] *= scale_x\n",
    "    bb2[1, :, :] *= scale_y\n",
    "\n",
    "    # 5. split substrings to cleaned words + per-word char counts\n",
    "    words       = split_and_clean(substrs)\n",
    "    char_counts = [len(w) for w in words]\n",
    "    if sum(char_counts) != bb2.shape[2]:\n",
    "        # fallback: treat entire line as one word\n",
    "        words       = [''.join(chars)]\n",
    "        char_counts = [len(chars)]\n",
    "\n",
    "    # 6. rebuild wordBB by grouping charBB\n",
    "    word_boxes = []\n",
    "    start = 0\n",
    "    for cnt in char_counts:\n",
    "        idxs = list(range(start, start+cnt))\n",
    "        xs   = bb2[0, :, idxs].flatten()\n",
    "        ys   = bb2[1, :, idxs].flatten()\n",
    "        x0, x1 = xs.min(), xs.max()\n",
    "        y0, y1 = ys.min(), ys.max()\n",
    "        # four corners: (x0,y0),(x1,y0),(x1,y1),(x0,y1)\n",
    "        word_boxes.append(np.array([[x0, x1, x1, x0],\n",
    "                                    [y0, y0, y1, y1]], dtype=float))\n",
    "        start += cnt\n",
    "    wb2 = np.stack(word_boxes, axis=2)  # shape (2,4,len(words))\n",
    "\n",
    "    # 7. save resized image\n",
    "    out_name = os.path.basename(name)\n",
    "    dst_path = os.path.join(IMG_DIR, out_name)\n",
    "    img_res.save(dst_path)\n",
    "\n",
    "    # 8. record\n",
    "    new_imnames.append(out_name)\n",
    "    new_charBB.append(bb2)\n",
    "    new_wordBB.append(wb2)\n",
    "    new_txt.append(words)\n",
    "    \n",
    "    # 9. limit the dataset generation. May cause the progress bar to end abruptly.\n",
    "    if MAX_IMAGES != -1 and len(new_imnames) >= MAX_IMAGES:\n",
    "        break\n",
    "\n",
    "#  end loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49078b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 20000 entries to datasets/SynthText_sub_resized_TESTIN\\gt.mat\n"
     ]
    }
   ],
   "source": [
    "N = len(new_imnames)\n",
    "im_arr   = np.empty((1, N), dtype=object)\n",
    "cbb_arr  = np.empty((1, N), dtype=object)\n",
    "wbb_arr  = np.empty((1, N), dtype=object)\n",
    "txt_arr  = np.empty((1, N), dtype=object)\n",
    "\n",
    "for i in range(N):\n",
    "    im_arr[0,i]  = new_imnames[i]\n",
    "    cbb_arr[0,i] = new_charBB[i]\n",
    "    wbb_arr[0,i] = new_wordBB[i]\n",
    "    txt_arr[0,i] = np.array(new_txt[i], dtype=object)\n",
    "\n",
    "savemat(RESIZED_GT, {\n",
    "    'imnames': im_arr,\n",
    "    'charBB':  cbb_arr,\n",
    "    'wordBB':  wbb_arr,\n",
    "    'txt':     txt_arr,\n",
    "}, do_compression=True)\n",
    "\n",
    "print(f\"Wrote {N} entries to {RESIZED_GT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473ca655",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
